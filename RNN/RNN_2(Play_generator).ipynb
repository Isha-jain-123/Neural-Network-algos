{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_2(Play generator).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MptOS4AmORwX"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG3B0H7VaGVm",
        "outputId": "5c0f34ff-692f-4f34-ad4c-fd4ee2308d70"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbdGLoAEaGom"
      },
      "source": [
        "# use this if you want to upload you're own text document , for now we're using shakespeare\n",
        "'''\n",
        "from google.colab import files\n",
        "path_to_file=list(files.upload().keys())[0]\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41AOTiZzaGre",
        "outputId": "5e4e2ada-6615-408c-a3bd-8603b7ff221a"
      },
      "source": [
        "text=open(path_to_file,'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "#length of the text is the number f characters in it\n",
        "print('Length of text : {} characters'.format(len(text)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text : 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUPjbHLlbEru"
      },
      "source": [
        "print(text[:300])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh1gFoZocS_H"
      },
      "source": [
        "**Encoding**\n",
        "\n",
        "Each unique character will be encoded with a different integer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AXoosxFbEtu"
      },
      "source": [
        "vocab=sorted(set(text))           # sorted all unique characters of the text as a list\n",
        "\n",
        "char2idx={u:i for i,u in enumerate(vocab)}\n",
        "idx2char=np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int=text_to_int(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANyYotWBbExD"
      },
      "source": [
        "print(\"Text: \", text[:13])\n",
        "print(\"Encoded: \", text_to_int(text[:13]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWC_sT19cn9-",
        "outputId": "a052d5e7-3508-4e93-92ac-4c175201651b"
      },
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints=ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFGL6Ry4e2Yq"
      },
      "source": [
        "Creating Training Examples\n",
        "\n",
        "We're going to give a sequence as input for training(not the entire text), and we'll give the labels as the same sequence shifted to the right by 1 unit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MffH2MoBcn_6"
      },
      "source": [
        "seq_length=100 \n",
        "examples_per_epoch=len(text)//(seq_length+1)\n",
        "\n",
        "# creating training examples/targets\n",
        "char_dataset=tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL5BFUgsf3c6"
      },
      "source": [
        "sequences=char_dataset.batch(seq_length+1,drop_remainder=True)   #putting the dataset into batches of desired length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKyHhE71iRi-"
      },
      "source": [
        "Now we will use those sequences of 101 length and split them into input and output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C_qNco_coD2"
      },
      "source": [
        "def split_input_target(chunk):  # say hello\n",
        "  input_text=chunk[:-1]         # hell\n",
        "  target_text=chunk[1:]         # ello\n",
        "  return input_text,target_text\n",
        "\n",
        "dataset=sequences.map(split_input_target)       # map is used to apply the above function to every entry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0B7tdhfiYmv",
        "outputId": "d7be796a-ff2d-44a3-eff6-bd7b13654d1f"
      },
      "source": [
        "for x,y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9v3AT1-kN8g"
      },
      "source": [
        "Making Training Batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIP2kBBTkA8M"
      },
      "source": [
        "BATCH_SIZE=64\n",
        "VOCAB_SIZE=len(vocab)\n",
        "EMBEDDING_DIM=256          # dimension of vectors in embedding layer\n",
        "RNN_UNITS=1024\n",
        "\n",
        "#buffer size to shuffle the elements\n",
        "BUFFER_SIZE=10000\n",
        "\n",
        "data=dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLvW7Npbk42B"
      },
      "source": [
        "**Building the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffxfkJ_ZiYpp",
        "outputId": "a092ba5d-d24f-4838-8e26-cb46835cc361"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model=tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size,embedding_dim,batch_input_shape=[batch_size,None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,return_sequences=True,\n",
        "                         stateful=True,recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)                        \n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model=build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPBG3XR8wghZ"
      },
      "source": [
        "Creating a Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU6ZQMw8coG_",
        "outputId": "42e9d6aa-a240-4eb4-8082-125dae92b6fc"
      },
      "source": [
        "for input_example_batch,target_example_baatch in data.take(1):\n",
        "  example_batch_predictions=model(input_example_batch)         #prediction on 1st batch\n",
        "  print(example_batch_predictions.shape, \"# (batch_size,sequence_length,vocab_size)\")       #output shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size,sequence_length,vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yfdd09nwbfF",
        "outputId": "df5cea7a-e260-41b3-cb4c-0501cd585325"
      },
      "source": [
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-4.35322337e-03 -5.96316718e-03  6.21498155e-04 ...  3.95013369e-04\n",
            "   -6.28710538e-03 -9.82082449e-04]\n",
            "  [-4.83203260e-03 -4.38144570e-03 -2.91602360e-03 ... -8.14423838e-04\n",
            "   -2.05587968e-03  1.07849506e-03]\n",
            "  [-8.50506965e-03 -8.61323997e-03 -8.97047110e-03 ... -1.11458125e-03\n",
            "    1.71006855e-03  3.99824185e-03]\n",
            "  ...\n",
            "  [-1.11014172e-02 -1.05707645e-02 -7.37554859e-03 ... -1.31539116e-03\n",
            "    2.21319683e-03  1.75144635e-02]\n",
            "  [-6.24691788e-03 -1.39642358e-02 -4.13247105e-03 ...  4.61515505e-04\n",
            "   -7.84620584e-04  1.24724451e-02]\n",
            "  [-4.68615489e-03 -9.03265644e-03 -5.08794514e-03 ... -9.38251615e-05\n",
            "    3.46405292e-03  1.59397833e-02]]\n",
            "\n",
            " [[-4.40974766e-03  5.57443919e-03  3.52374371e-03 ...  1.95894716e-03\n",
            "    1.94849411e-03 -2.75645498e-03]\n",
            "  [-6.44375756e-03  4.97705815e-03 -4.72289074e-04 ... -5.91672142e-04\n",
            "    3.82385333e-03  7.59961782e-04]\n",
            "  [-8.88469163e-03  9.09097679e-03  3.25556146e-03 ...  1.59917842e-03\n",
            "    4.28472925e-03 -2.46180221e-04]\n",
            "  ...\n",
            "  [ 2.91896402e-03  3.93451378e-03 -3.85855325e-03 ...  1.46482768e-03\n",
            "    3.14886193e-03  1.43411541e-02]\n",
            "  [ 2.74977111e-03  5.32435486e-03 -2.87147705e-03 ... -1.19682332e-03\n",
            "   -1.04400067e-04  1.22771068e-02]\n",
            "  [ 1.43606146e-03  2.58264714e-03 -5.54808509e-03 ...  4.63979412e-03\n",
            "   -3.07916291e-03  1.32167311e-02]]\n",
            "\n",
            " [[-3.83964763e-03 -1.43698556e-03 -4.64674691e-03 ... -8.07105843e-03\n",
            "   -2.82416632e-03  5.43873338e-03]\n",
            "  [-3.94528732e-03 -2.53932225e-03 -6.77955756e-03 ... -2.57363240e-03\n",
            "   -7.07222521e-03  5.55877434e-03]\n",
            "  [-4.56493394e-03  2.07427563e-03 -6.72421232e-03 ... -8.02616868e-03\n",
            "   -1.47360924e-03  9.80428141e-03]\n",
            "  ...\n",
            "  [ 4.55716159e-04  6.06337981e-03 -4.29206900e-03 ...  5.93572273e-04\n",
            "    1.40276272e-04  1.29584875e-02]\n",
            "  [-3.54307378e-03  4.28270549e-03 -6.80843182e-03 ... -1.96535955e-03\n",
            "    3.29110026e-03  1.36677492e-02]\n",
            "  [-7.28832185e-03  7.58734206e-03 -1.73282064e-03 ...  1.40385237e-04\n",
            "    4.64262441e-03  1.03847310e-02]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-6.40575076e-03 -4.62490041e-03 -6.74267719e-03 ... -1.21328328e-03\n",
            "    3.52527457e-03  1.97271910e-03]\n",
            "  [-8.87475349e-03 -4.41908278e-03 -9.60809086e-03 ... -8.19241535e-03\n",
            "   -9.44148749e-04  6.63351500e-03]\n",
            "  [-9.21841711e-03 -4.60303435e-03 -1.52970920e-03 ... -1.34068367e-04\n",
            "   -2.00878689e-03  5.06888609e-03]\n",
            "  ...\n",
            "  [-4.26296750e-03  1.49711117e-03 -8.68126377e-03 ... -6.72413409e-03\n",
            "    5.38827712e-03  6.37143804e-03]\n",
            "  [-7.38448184e-03  2.33744434e-03 -9.41900909e-03 ... -5.62905055e-03\n",
            "    5.39587252e-03  7.80280028e-03]\n",
            "  [-2.20714579e-03 -3.76564334e-03 -6.09923853e-03 ... -2.18368159e-03\n",
            "    1.97336380e-03  5.16859069e-03]]\n",
            "\n",
            " [[-4.40974766e-03  5.57443919e-03  3.52374371e-03 ...  1.95894716e-03\n",
            "    1.94849411e-03 -2.75645498e-03]\n",
            "  [-5.40408492e-03  2.94818520e-03 -1.57175516e-03 ...  5.41595230e-03\n",
            "   -4.26974962e-04  2.11136439e-03]\n",
            "  [-7.26451352e-03 -1.28129683e-03 -2.59741000e-03 ...  2.44828966e-03\n",
            "    9.65253217e-04 -2.61069974e-04]\n",
            "  ...\n",
            "  [-1.64014846e-02  9.11597535e-03 -9.58758127e-03 ... -2.03544903e-03\n",
            "    3.05878324e-03  1.17846876e-02]\n",
            "  [-9.97174252e-03  1.20653547e-02 -1.09911561e-02 ... -2.40633893e-03\n",
            "    3.23682395e-03  8.96213017e-03]\n",
            "  [-9.88901034e-03  5.44393249e-03 -9.26612969e-03 ... -2.61920528e-03\n",
            "    6.65718736e-03  3.65133141e-03]]\n",
            "\n",
            " [[-1.99317560e-03  6.79698423e-04 -3.65054444e-03 ... -1.42914313e-03\n",
            "    2.22404674e-03  1.69470464e-03]\n",
            "  [-1.65793602e-03 -3.48219275e-03 -4.57873475e-03 ... -2.09920667e-03\n",
            "    4.70314641e-03 -2.78809830e-03]\n",
            "  [-3.63860209e-03  2.53521558e-03 -8.39884335e-04 ...  1.09537900e-03\n",
            "    4.66176681e-03 -3.73776956e-03]\n",
            "  ...\n",
            "  [-8.68534669e-03 -2.22313777e-03 -7.13762827e-03 ...  4.62949323e-03\n",
            "   -8.70987587e-03  7.82262627e-03]\n",
            "  [-9.17890575e-03 -1.00289239e-03 -1.00403614e-02 ...  7.92543869e-03\n",
            "   -8.77575949e-03  1.07380189e-02]\n",
            "  [-1.22312978e-02  6.18831115e-03 -3.92607599e-03 ...  7.01656332e-03\n",
            "   -5.80954365e-03  6.11793809e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrUMsZ1mS6NM",
        "outputId": "ae04f5c4-c181-47b2-d25f-375b7845bbaf"
      },
      "source": [
        "# examining one prediction\n",
        "pred=example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-4.3532234e-03 -5.9631672e-03  6.2149815e-04 ...  3.9501337e-04\n",
            "  -6.2871054e-03 -9.8208245e-04]\n",
            " [-4.8320326e-03 -4.3814457e-03 -2.9160236e-03 ... -8.1442384e-04\n",
            "  -2.0558797e-03  1.0784951e-03]\n",
            " [-8.5050697e-03 -8.6132400e-03 -8.9704711e-03 ... -1.1145812e-03\n",
            "   1.7100686e-03  3.9982419e-03]\n",
            " ...\n",
            " [-1.1101417e-02 -1.0570765e-02 -7.3755486e-03 ... -1.3153912e-03\n",
            "   2.2131968e-03  1.7514464e-02]\n",
            " [-6.2469179e-03 -1.3964236e-02 -4.1324710e-03 ...  4.6151550e-04\n",
            "  -7.8462058e-04  1.2472445e-02]\n",
            " [-4.6861549e-03 -9.0326564e-03 -5.0879451e-03 ... -9.3825161e-05\n",
            "   3.4640529e-03  1.5939783e-02]], shape=(100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "za_tlVBqTdeK",
        "outputId": "d0527dab-759b-4764-8554-917f6c8360ab"
      },
      "source": [
        "# now we'll look at a prediction at the first timestep\n",
        "time_pred=pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# the 65 values represent the probabilities of each charater occuring next"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-4.3532234e-03 -5.9631672e-03  6.2149815e-04 -2.9003234e-03\n",
            "  5.3185475e-04 -7.8967959e-04  6.2883417e-03 -4.5606815e-03\n",
            " -2.5071555e-03 -9.2325509e-03  8.9550321e-04 -4.5694583e-03\n",
            " -4.2928578e-03 -3.1235784e-03  1.0440724e-03 -5.4096943e-04\n",
            "  9.2637405e-04  3.9796107e-03 -2.7889125e-03 -3.3533820e-03\n",
            " -3.2813624e-03  5.2086795e-03  4.4036568e-03 -3.9666593e-03\n",
            "  1.6528995e-03  3.2263470e-04  1.4312507e-03  2.7853320e-04\n",
            " -2.5721374e-03 -1.4299988e-03 -8.4274582e-04  2.0258185e-03\n",
            "  6.1624189e-05  8.5192733e-03 -7.5394055e-05 -1.1111780e-03\n",
            "  3.2387360e-04 -9.2128254e-03 -3.2711274e-03  1.6333652e-03\n",
            "  1.3942006e-03 -2.1381353e-03  4.0205340e-03  6.5847137e-03\n",
            "  1.4907643e-03 -5.2932780e-03 -4.9783862e-03 -6.0028774e-03\n",
            " -4.6517234e-03  1.2986710e-03  2.7053084e-03  1.1229775e-03\n",
            "  3.2225118e-03 -1.0363717e-03  4.1581118e-03  3.0091717e-03\n",
            " -1.4300719e-03 -3.5022711e-03 -1.1557562e-04 -1.4216393e-03\n",
            " -9.3663606e-05  6.0275284e-04  3.9501337e-04 -6.2871054e-03\n",
            " -9.8208245e-04], shape=(65,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "A8e456zgTdg-",
        "outputId": "81371173-de6d-4a9f-a68e-50bae6507296"
      },
      "source": [
        "sampled_indices=tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "sampled_indices=np.reshape(sampled_indices, (1,-1))[0]\n",
        "predicted_chars=int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Mkl$WEBXR!\\nkKcwpHLQ.KR:a?knqtxWQRmusOhmdinMhg-uAYd'UiToqTnvbeT3Tmo dkqskzTJpfqYAdUhM'E3MupwSkLazP,Va\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpARCCgpUAXd"
      },
      "source": [
        "def loss(labels,logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels,logits,from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDf6041xWSjo"
      },
      "source": [
        "Compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZtUCKHsWRaF"
      },
      "source": [
        "model.compile(optimizer='adam',loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvo3pKf5Wnh8"
      },
      "source": [
        "Creating Checkpoints\n",
        "\n",
        "Now we will set up our model to save checkpoints as it trains. This will allow us to load our model from a checkpoint and continue training it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkm_oC9ZWRcc"
      },
      "source": [
        "checkpoint_dir=\"./training_checkpoints\" # directory where it'll be saved\n",
        "#Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYv4P6W2XpHX"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r493PbzTWRfo",
        "outputId": "4f449bc4-81e6-4f9e-c1d8-d0e1f9fef0cb"
      },
      "source": [
        "history=model.fit(data, epochs=40, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "172/172 [==============================] - 13s 63ms/step - loss: 2.5974\n",
            "Epoch 2/40\n",
            "172/172 [==============================] - 12s 64ms/step - loss: 1.8935\n",
            "Epoch 3/40\n",
            "172/172 [==============================] - 12s 64ms/step - loss: 1.6385\n",
            "Epoch 4/40\n",
            "172/172 [==============================] - 12s 65ms/step - loss: 1.5034\n",
            "Epoch 5/40\n",
            "172/172 [==============================] - 12s 66ms/step - loss: 1.4222\n",
            "Epoch 6/40\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 1.3653\n",
            "Epoch 7/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.3195\n",
            "Epoch 8/40\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 1.2796\n",
            "Epoch 9/40\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 1.2437\n",
            "Epoch 10/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.2077\n",
            "Epoch 11/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.1722\n",
            "Epoch 12/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.1345\n",
            "Epoch 13/40\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 1.0977\n",
            "Epoch 14/40\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 1.0589\n",
            "Epoch 15/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.0180\n",
            "Epoch 16/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.9770\n",
            "Epoch 17/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.9361\n",
            "Epoch 18/40\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.8948\n",
            "Epoch 19/40\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.8554\n",
            "Epoch 20/40\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.8170\n",
            "Epoch 21/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.7811\n",
            "Epoch 22/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.7473\n",
            "Epoch 23/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.7170\n",
            "Epoch 24/40\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.6869\n",
            "Epoch 25/40\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.6603\n",
            "Epoch 26/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.6374\n",
            "Epoch 27/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.6156\n",
            "Epoch 28/40\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.5955\n",
            "Epoch 29/40\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.5771\n",
            "Epoch 30/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5637\n",
            "Epoch 31/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5482\n",
            "Epoch 32/40\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.5355\n",
            "Epoch 33/40\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.5239\n",
            "Epoch 34/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5127\n",
            "Epoch 35/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5036\n",
            "Epoch 36/40\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4944\n",
            "Epoch 37/40\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4852\n",
            "Epoch 38/40\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4795\n",
            "Epoch 39/40\n",
            "172/172 [==============================] - 12s 68ms/step - loss: 0.4728\n",
            "Epoch 40/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbu7KmdFbEqR"
      },
      "source": [
        "Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8Xpbcv5S6QM"
      },
      "source": [
        "model=build_model(VOCAB_SIZE,EMBEDDING_DIM,RNN_UNITS,batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRI83-L1wbiU"
      },
      "source": [
        "# for getting the most recent checkpoint\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAt5Qnc5bsWE"
      },
      "source": [
        "# for getting a checkpoint of a specific epoch\n",
        "'''\n",
        "checkpoint_num=10\n",
        "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\"+str(checkpoint_num))\n",
        "model.build(tf.TensorShape([1,None]))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uvSD9K2da1_"
      },
      "source": [
        "**GENERATING TEXT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e394cXSGbsYV"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  num_generate=800\n",
        "\n",
        "  input_eval=[char2idx[s] for s in start_string]\n",
        "  input_eval=tf.expand_dims(input_eval,0)\n",
        "\n",
        "  text_generated=[]\n",
        "\n",
        "  temperature=1.0     # low temperature gives more predictable text, high gives surprising texts\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions=model(input_eval)\n",
        "\n",
        "    predictions=tf.squeeze(predictions,0)\n",
        "\n",
        "    predictions=predictions/temperature\n",
        "    predicted_id=tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    input_eval=tf.expand_dims([predicted_id],0)\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return start_string+''.join(text_generated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML5ZBZ7dbsdC",
        "outputId": "8493a6aa-27d0-4bc8-c5c5-987b47ff3bcc"
      },
      "source": [
        "inp=input(\"Type a starting string\")\n",
        "print(generate_text(model,inp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type a starting stringRomeo\n",
            "Romeou chides; thit has he doth awhile,\n",
            "And most all dead, that lift such ventle deeds,\n",
            "They have been still begin to dry have your names:\n",
            "Provost, a fool, issue, which was so abrace?\n",
            "\n",
            "First Murderer:\n",
            "Now, in good time: how love to her King of\n",
            "Eath, strings musician in the sun:\n",
            "So shall you be a Richard, I am pains\n",
            "For this most precedee.\n",
            "I should have done thy grey have need of much ado.\n",
            "\n",
            "KING RICHARD III:\n",
            "I will not miss the play so. Where is her high me ha?\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Why, York, what wilt thou do?\n",
            "\n",
            "GREMIO:\n",
            "Northumberland, redees him hither; and so wide as deeply\n",
            "Letturders:\n",
            "Look that by God's fance unbrothed i' the body.\n",
            "\n",
            "MENENIUS:\n",
            "I am a great n particular saint,\n",
            "Were tender y an imprisonment in foul swift and masks.\n",
            "I confess you,\n",
            "Let them have given to beat by the shepherd;\n",
            "Reform'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKEbWbsRgjsy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKT8x9hpbshx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}